# -*- coding: utf-8 -*-
"""GenAI Capstone - Short Stories to Visualizer

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/pitchet2/genai-capstone-short-stories-to-visualizer.841a449b-d0d0-4118-9f70-078db12fec09.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250725/auto/storage/goog4_request%26X-Goog-Date%3D20250725T002330Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D5a91c6822f0137545360bd684d294f38900c322b761781e11943f3cc3eaf0bc0a275c2d5cf300c2d89e89585c5be8db6aec9ae1d89d23fda292db35c4c6f6a16e685b7509753f10a3bd54581a839eb7667c26acaa4427b6db98e973e86ed4b8dfcb11a24545af1d80802528b1070055149ad8c131e79c088abf414ff3e7a6e274116a5ce613eea3102cc209a292fb9c1d423c4d338604ea8347139908e386d9209ad28174971f94e4ee6d98a352262b77c945ffba56ff74cac16567a98a0677b481a316a89a470fb160201d1a7a5e533fdb7d99fb3e6aebdfb6168397be9cfb68490c9398045d797329651b0e08858739ccb148931f3086083859decba242af0
"""

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
import kagglehub
kagglehub.login()

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

gen_ai_intensive_course_capstone_2025q1_path = kagglehub.competition_download('gen-ai-intensive-course-capstone-2025q1')

print('Data source import complete.')

"""# Project Introduction: AI Story Visualizer with LangGraph and Gemini

This Jupyter Notebook that demonstrates a workflow for transforming short stories into basic audio-visual experiences using Generative AI. By leveraging the power of Google's Gemini models for text processing and generation, coupled with Google's image creation capabilities, the TTS model for text-to-speech, and the orchestration prowess of LangGraph, this project automates the creation of visual narratives from plain text.

## üí° Use Case and Innovation

This section details the motivation, novelty, and potential impact of this automated story visualization workflow.

### Use Case

The primary objective is to take a simple text-based short story and automatically generate a corresponding sequence of images and synchronized audio narration. Each logical scene identified within the story will be represented by a unique image and an accompanying audio clip. This provides a rapid and efficient way to prototype visual adaptations of textual content.

### Innovation & Novelty

The core innovation of this project lies in its fully automated, end-to-end conversion process, expertly managed by LangGraph. This framework ensures a consistent visual and narrative style (including character descriptions) across the generated assets, a feat that would be significantly more complex and time-consuming to achieve manually. By seamlessly integrating multiple cutting-edge AI capabilities ‚Äì Natural Language Understanding (NLU), text generation, image synthesis, and Text-to-Speech (TTS) ‚Äì into a structured pipeline, this project showcases a novel approach to content creation.

### Impact

* **Storyboarding and Pre-visualization:** Quickly generating visual representations of story ideas.
* **Content Creation:** Streamlining the initial asset generation phase for multimedia projects.
* **Accessibility:** Creating basic audio-visual versions of text to enhance accessibility for individuals with reading difficulties.
* **Educational Tools:** Developing engaging and interactive learning materials.

### GenAI Suitability

This use case is ideally suited for Generative AI due to the inherent requirements for:

* **Natural Language Understanding:** To effectively parse the story, identify key elements like characters and scenes, and understand the overall tone and style.
* **Generative Models:** To create original content in the form of character descriptions, image prompts, the images themselves, and the audio narration.
* **Workflow Orchestration:** LangGraph provides the crucial framework to effectively chain these diverse GenAI capabilities into a coherent and automated process.

## ‚úÖ Demonstrated GenAI Capabilities

This workflow effectively demonstrates several key Generative AI capabilities:

* **Structured Output/JSON Mode/Controlled Generation:** The `analyze_characters` and `analyze_scenes` nodes utilize the LLM's ability to return information in a predefined JSON format, ensuring structured and predictable output for downstream tasks.
* **Document Understanding:** Multiple nodes within the workflow (`analyze_characters`, `analyze_scenes`, `determine_overall_style`) demonstrate the ability to read and interpret the input `story_text` to extract relevant information and make informed decisions.
* **Agents (via LangGraph):** The entire LangGraph setup functions as an intelligent agent, orchestrating a series of steps and tool calls (including LLMs, image generation, and TTS) based on a clearly defined flow and the current state of the processing.
* **Long Context Window Utilization:** The workflow relies on the underlying Gemini model's capacity to process the entire story text (or substantial portions thereof) within its context window during the analysis phases, enabling a holistic understanding of the narrative.

## Project Outline

**I. Character Appearance Management**
    * a. Determine character appearances based on the story text.
    * b. If no explicit description exists, create a general appearance for each character.
    * c. Keep track of character appearances to ensure consistency in image generation.

**II. Scene Breakdown and Tone Analysis**
    * a. Break down the story into individual scenes.
    * b. Identify the tone of each scene (e.g., fun, serious). Can be used in the image/audio generation.

**III. Scene Details Identification**
    * a. Identify the scenery for each scene.
    * b. Identify the characters present in each scene.

**IV. Image Generation**
    * a. Generate an image for each scene.
    * b. Utilize character appearance information from Step I.
    * c. Utilize scenery and character presence information from Step III.

**V. Audio Generation**
    * a. Generate audio for each scene.
    * b. Match the audio tone to the scene tone identified in Step II.

**VI. Media Combination**
    * a. Combine the generated audio and images for each scene.
    * b. Ensure the combined elements create a coherent story flow.

## Install dependencies
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# from IPython.display import clear_output
# # Remove conflicting packages from the Kaggle base environment.
# !pip uninstall -qqy kfp jupyterlab libpysal thinc spacy fastai ydata-profiling google-cloud-bigquery google-generativeai
# # Install langgraph and the packages
# !pip install -qU 'langgraph==0.3.21' 'langchain-google-genai==2.1.2' 'langgraph-prebuilt==0.1.7'
# !pip install TTS
# !pip install transformers==4.46.1
# !pip install -q accelerate
# clear_output()

"""## Import libraries"""

# --- Dependencies ---

import operator
import json
import os
from typing import Annotated, TypedDict, List, Dict, Optional
import io
import time # Added for delays gemini-2.0-flash has rate limits of 10 per min
import re
import nltk

# Langchain/LangGraph specific imports
from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_google_genai import ChatGoogleGenerativeAI # For text generation
from langgraph.graph import StateGraph, END

# Google genai
from google import genai
from google.genai import types
from PIL import Image
import numpy as np
import pandas as pd
import torch
from scipy.io.wavfile import write

# Kaggle
from kaggle_secrets import UserSecretsClient

# torch
import torch
import gc

from TTS.api import TTS

# Get device
device = "cuda" if torch.cuda.is_available() else "cpu"

"""# Initialized models"""

# --- TTS model ---
device = "cuda" if torch.cuda.is_available() else "cpu"
tts = TTS(model_name="tts_models/en/ljspeech/tacotron2-DDC_ph", progress_bar=False).to(device)

# --- Configuration ---

# Set your Google API Key environment variable
GOOGLE_API_KEY = UserSecretsClient().get_secret("GOOGLE_API_KEY")
os.environ["GOOGLE_API_KEY"] = GOOGLE_API_KEY

# Check if the API key is set
google_api_key = os.getenv("GOOGLE_API_KEY")
if not google_api_key:
    print("‚ö†Ô∏è Warning: GOOGLE_API_KEY environment variable not set.")
else:
    print(f"GOOGLE_API_KEY loaded")


# Initialize the LLM for text tasks (LangChain integration)
# Using "gemini-2.0-flash"
try:
    llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash", temperature=0.7)
    print("LLM (LangChain) initialized with model: gemini-2.0-flash")
except Exception as e:
    print(f"Error initializing LLM with model gemini-2.0-flash: {e}")
    print("Please ensure your GOOGLE_API_KEY is set correctly and the model is available.")

# Initialize the Google GenAI Client for direct image generation use
try:
    # Ensure genai is configured before creating the client if needed by the library version
    if google_api_key: # Attempt client init only if key exists
         genai_client = genai.Client()
         print("Google GenAI Image Generation client initialized (using genai.Client).")
    else:
         print("‚ö†Ô∏è Skipping Google GenAI client initialization due to missing API key.")
         genai_client = None

except Exception as e:
    print(f"Error initializing Google GenAI Image Generation client: {e}")
    genai_client = None

"""# Building Graph
## Defined Graph State
"""

# --- 1. Define Graph State ---

# Define the structure for individual scene information
class SceneInfo(TypedDict):
    scene_number: int
    scene_text: str # Full text of the scene for audio generation
    summary: str # Brief summary of the scene's content/action
    setting: str # Description of the location/environment
    characters_present: List[str] # List of character names in the scene
    tone: str # Overall mood or tone (e.g., mysterious, calm, frantic, joyful) to be used in TTS model if available
    image_prompt: Optional[str] # Generated prompt for image model
    image_url: Optional[str] # File path to the generated image
    audio_url: Optional[str] # File path to the generated audio

# Define the structure for story analysis
class StoryAnalysisState(TypedDict):
    """
    Represents the state of our story analysis graph.
    """
    story_text: str # The original short story
    characters: Dict[str, Dict[str, str]] # Key: Character Name, Value: Dict of attributes (e.g., 'description')
    scenes: List[SceneInfo] # List of scenes, structured using SceneInfo
    overall_style: Optional[str] # Suggested overall visual style for images
    processing_log: List[str] # To track the steps
    # Add potential config for image/audio generation API if needed later
    # image_gen_config: Optional[Dict]
    # audio_gen_config: Optional[Dict]

# --- 2. Define Nodes (Functions) ---

def read_story(state: StoryAnalysisState) -> StoryAnalysisState:
    """
    Node to load the story text into the state.
    """
    log = state.get("processing_log", [])
    log.append("Reading story...")
    print("--- Reading Story ---")
    return {"processing_log": log}

def analyze_characters(state: StoryAnalysisState) -> StoryAnalysisState:
    """
    Node to identify characters and their initial descriptions using Gemini (via Langchain).
    """
    log = state.get("processing_log", [])
    log.append("Analyzing characters using Gemini...")
    print("--- Analyzing Characters (using Gemini) ---")
    story = state["story_text"]
    characters_found = {}


    prompt_template = ChatPromptTemplate.from_messages([
        ("system", """You are an expert literary analyst. Your task is to read the provided story text and identify all named characters.
For each character, extract any description of their appearance, or notable features mentioned directly in the text.
Output the results as a JSON object where keys are the character names and values are dictionaries containing a single key "description" with the extracted description as a string.
If no description is found for a character, set the value of "description" to null or an empty string.

Example JSON output format:
{{
  "Character Name 1": {{"description": "Description found in text."}},
  "Character Name 2": {{"description": null}}
}}
"""),
        ("human", "Please analyze the following story text:\n\n{story_text}")
    ])
    parser = JsonOutputParser()
    chain = prompt_template | llm | parser

    try:
        response = chain.invoke({"story_text": story})
        characters_found = response
        # Ensure description is always a string (replace null with empty string if needed)
        for char, details in characters_found.items():
            if details is not None and details.get("description") is None:
                 characters_found[char]["description"] = ""
            # Handle cases where the character value itself might be None from JSON
            elif details is None:
                 characters_found[char] = {"description": ""}
        log.append(f"Successfully analyzed characters. Found: {list(characters_found.keys())}")
        print(f"Found characters: {characters_found}")
    except Exception as e:
        log.append(f"Error analyzing characters: {e}")
        print(f"Error during character analysis: {e}")

    return {"characters": characters_found, "processing_log": log}


def generate_missing_descriptions(state: StoryAnalysisState) -> StoryAnalysisState:
    """
    Node to generate descriptions for characters identified without one using Gemini (via Langchain).
    """
    log = state.get("processing_log", [])
    log.append("Checking for and generating missing descriptions using Gemini...")
    print("--- Generating Missing Descriptions (using Gemini) ---")
    characters = state.get("characters", {})
    story = state["story_text"]
    updated_characters = characters.copy()


    prompt_template = ChatPromptTemplate.from_messages([
        ("system", """You are a creative character designer. Based on the character's name and the provided story context, generate a brief, plausible physical description (appearance, clothing, general impression) for the character.
Focus on visual details suitable for image generation later.
Output ONLY the generated description as a plain string."""),
        ("human", "Generate a description for the character '{character_name}' based on this story context:\n\n{story_context}")
    ])
    parser = StrOutputParser()
    chain = prompt_template | llm | parser

    characters_to_generate = []
    if isinstance(characters, dict):
        for name, details in characters.items():
            # Check if details is a dict and if description is missing/empty
            if isinstance(details, dict) and not details.get("description", "").strip():
                characters_to_generate.append(name)
            # Handle cases where the character entry might not be a dict (e.g., due to parsing error)
            elif not isinstance(details, dict):
                 log.append(f"Skipping description generation for '{name}' due to unexpected format: {details}")
                 print(f"Warning: Skipping description generation for '{name}' due to unexpected format.")
                 # Ensure the entry is a dict for consistency, even if description remains missing
                 updated_characters[name] = {"description": ""}


    if not characters_to_generate:
        log.append("No missing descriptions to generate.")
        print("No missing descriptions to generate.")
        # Ensure the returned state always includes the characters dict
        return {"characters": updated_characters, "processing_log": log}

    print(f"Attempting to generate descriptions for: {', '.join(characters_to_generate)}")
    for name in characters_to_generate:
        time.sleep(8) #Gemini-2.0-flash has rate limit of 10 per minute
        try:
            # Invoke the chain
            generated_desc = chain.invoke({
                "character_name": name,
                "story_context": story # Provide the full story as context
            })
            # Ensure the character exists in updated_characters before assigning
            if name not in updated_characters:
                 updated_characters[name] = {} # Initialize if somehow missing
            updated_characters[name]["description"] = generated_desc.strip()
            log.append(f"Successfully generated description for {name}.")
            print(f"Generated description for {name}: {generated_desc.strip()}")
        except Exception as e:
            log.append(f"Error generating description for {name}: {e}")
            print(f"Error generating description for {name}: {e}")
            # Optionally set a default error description
            if name not in updated_characters:
                 updated_characters[name] = {}
            updated_characters[name]["description"] = "Error generating description."

    return {"characters": updated_characters, "processing_log": log}


def analyze_scenes(state: StoryAnalysisState) -> StoryAnalysisState:
    """
    Node to break the story into scenes and extract details for each using Gemini (via Langchain).
    """
    log = state.get("processing_log", [])
    log.append("Analyzing scenes using Gemini...")
    print("--- Analyzing Scenes (using Gemini) ---")
    story = state["story_text"]
    character_names = list(state.get("characters", {}).keys())
    scenes_found = []

    prompt_template = ChatPromptTemplate.from_messages([
        ("system", """You are a skilled screenwriter and story analyst. Your task is to read the provided story text and divide it into logical scenes based on changes in location, time, or main character focus.

For each scene you identify, provide the following details:
1.  `scene_number`: An integer starting from 1.
2.  `scene_text`: A full text from the scene directly from the story.
3.  `summary`: A brief 1-2 sentence summary of the main action or content of the scene.
4.  `setting`: A short description of the scene's location and environment (e.g., 'sunny river bank', 'dark forest path', 'inside a rabbit hole').
5.  `characters_present`: A list of names of the characters who are actively present in the scene. Use the provided list of known character names: {character_list}. If other characters seem present, include their names too.
6.  `tone`: A single word or short phrase describing the overall mood or tone of the scene (e.g., 'calm', 'curious', 'frantic', 'mysterious', 'tense', 'confusing').

Output the results as a JSON list, where each element in the list is an object representing a scene with the keys 'scene_number', 'scene_text', 'summary', 'setting', 'characters_present', and 'tone'.

Example JSON output format:
[
  {{
    "scene_number": 1,
    "scene_text": "A wakes up to a new day in his tiny and messy bedroom, but A feels refresh, looking forward to go on this adventures.",
    "summary": "Character A wakes up and prepares for their journey.",
    "setting": "Small, cluttered bedroom",
    "characters_present": ["Character A"],
    "tone": "calm"
  }},
  {{
    "scene_number": 2,
    "scene_text": "A was wandering through the forest, the sun shinning through the leaves. A enjoyed his time in the wood when he ran into B, B is supposed to be sick.",
    "summary": "Character A meets Character B while walking through the forest.",
    "setting": "Sun-dappled forest path",
    "characters_present": ["Character A", "Character B"],
    "tone": "curious"
  }}
]
"""),
        ("human", "Please analyze the following story text and identify the scenes:\n\n{story_text}\n\nKnown character names: {character_list_str}")
    ])
    parser = JsonOutputParser()
    chain = prompt_template | llm | parser

    try:
        response = chain.invoke({
            "story_text": story,
            "character_list": character_names,
            "character_list_str": ", ".join(character_names)
        })
        # Initialize added fields
        scenes_found = []
        for scene_data in response:
             # Ensure basic structure even if LLM hallucinates extra fields
             validated_scene_data = {
                 'scene_number': scene_data.get('scene_number'),
                 'scene_text': scene_data.get('scene_text'),
                 'summary': scene_data.get('summary'),
                 'setting': scene_data.get('setting'),
                 'characters_present': scene_data.get('characters_present', []),
                 'tone': scene_data.get('tone'),
                 'image_prompt': None,
                 'image_url': None,
                 'audio_url': None
             }
             scenes_found.append(validated_scene_data)


        log.append(f"Successfully analyzed scenes. Found {len(scenes_found)} scenes.")
        print(f"Found {len(scenes_found)} scenes.")

    except Exception as e:
        log.append(f"Error analyzing scenes: {e}")
        print(f"Error during scene analysis: {e}")
        scenes_found = [] # Ensure it's an empty list on error

    return {"scenes": scenes_found, "processing_log": log}


def determine_overall_style(state: StoryAnalysisState) -> StoryAnalysisState:
    """
    Node to determine an overall visual style based on the story text using Gemini for consistent style for image generation.
    """
    log = state.get("processing_log", [])
    log.append("Determining overall visual style using Gemini...")
    print("--- Determining Overall Visual Style (using Gemini) ---")
    story = state["story_text"]
    suggested_style = None # Initialize

    # Prompt to suggest a style
    prompt_template = ChatPromptTemplate.from_messages([
        ("system", """You are an expert art director. Based on the overall tone, genre, setting, and content of the following story, suggest a concise visual style descriptor suitable for guiding an image generation model.
Examples: 'children's book illustration', 'dark fantasy oil painting', 'realistic sci-fi render', 'vintage cartoon style', 'photorealistic, cinematic lighting'.
Output ONLY the suggested style descriptor string, prefixed with a space (e.g., ' children's book illustration')."""),
        ("human", "Determine a visual style for this story:\n\n{story_text}")
    ])

    parser = StrOutputParser()
    chain = prompt_template | llm | parser

    try:
        # Invoke the chain with the story text
        suggested_style = chain.invoke({"story_text": story}).strip()
        # Basic validation: ensure it starts with a space
        if suggested_style and not suggested_style.startswith(" "):
             suggested_style = " " + suggested_style # Add prefix if missing

        log.append(f"Suggested overall style: '{suggested_style}'")
        print(f"Suggested overall style: '{suggested_style}'")

    except Exception as e:
        log.append(f"Error determining overall style: {e}")
        print(f"Error determining overall style: {e}")
        suggested_style = " illustration" # Fallback style on error

    # Ensure a fallback style if suggestion is empty or just the prefix
    if not suggested_style or suggested_style == " ":
        suggested_style = " illustration" # Default fallback style
        log.append(f"Using fallback style: '{suggested_style}'")
        print(f"Using fallback style: '{suggested_style}'")


    return {"overall_style": suggested_style, "processing_log": log}


def generate_image_prompts(state: StoryAnalysisState) -> StoryAnalysisState:
    """
    Node to generate image prompts for each scene using Gemini (via Langchain),
    including descriptions of characters present and adding the determined overall style descriptor.
    """
    log = state.get("processing_log", [])
    log.append("Generating image prompts using Gemini...")
    print("--- Generating Image Prompts (using Gemini) ---")
    scenes = state.get("scenes", [])
    characters_info = state.get("characters", {})
    # Get the determined overall style from the state
    overall_style = state.get("overall_style")
    updated_scenes = []

    if not llm:
        log.append("LLM not available. Skipping image prompt generation.")
        print("LLM not available. Skipping image prompt generation.")
        return {"scenes": scenes, "processing_log": log}

    # Use a default style if none was determined or passed
    if not overall_style:
        overall_style = " illustration" # Default fallback style
        log.append(f"Overall style not found in state, using fallback: '{overall_style}'")
        print(f"Overall style not found in state, using fallback: '{overall_style}'")

    print(f"Using overall style for prompts: '{overall_style}'")

    prompt_template = ChatPromptTemplate.from_messages([
        ("system", """You are an expert prompt engineer for text-to-image models.
Based on the provided scene details, create a concise yet descriptive prompt suitable for generating an image.
The prompt should capture the essence of the scene: the environment, characters (incorporating their descriptions), actions, and mood/tone.
Focus on visual elements. Output ONLY the generated prompt as a plain string.
"""),
        ("human", """Generate an image prompt for the following scene:
Scene Number: {scene_number}
Summary: {summary}
Setting: {setting}
Characters Present: {characters_present}
Relevant Character Descriptions:
{relevant_character_descriptions}
Tone: {tone}""")
    ])

    parser = StrOutputParser()
    chain = prompt_template | llm | parser

    for scene in scenes:
        scene_num = scene.get('scene_number', 'N/A')
        try:
            present_char_names = scene.get('characters_present', [])
            relevant_descriptions_list = []
            if isinstance(characters_info, dict):
                for name in present_char_names:
                    char_data = characters_info.get(name)
                    if isinstance(char_data, dict):
                        description = char_data.get('description', '').strip()
                        if description:
                            relevant_descriptions_list.append(f"- {name}: {description}")
                        else:
                            relevant_descriptions_list.append(f"- {name}: (No specific description provided)")
                    else:
                         relevant_descriptions_list.append(f"- {name}: (Character data missing)")

            if not relevant_descriptions_list:
                 relevant_desc_text = "None specific to this scene."
            else:
                 relevant_desc_text = "\n".join(relevant_descriptions_list)

            # Get the base content prompt from the LLM
            image_prompt_content = chain.invoke({
                "scene_number": scene_num,
                "summary": scene.get('summary', ''),
                "setting": scene.get('setting', ''),
                "characters_present": ", ".join(present_char_names) if present_char_names else "None",
                "relevant_character_descriptions": relevant_desc_text,
                "tone": scene.get('tone', '')
            })

            # Append the determined overall_style
            final_image_prompt = f"{image_prompt_content.strip()}. In the following style {overall_style}"

            # Store the final prompt (content + style)
            scene['image_prompt'] = final_image_prompt
            log.append(f"Generated image prompt for scene {scene_num}.")
            # Print the final prompt being used
            print(f"Generated image prompt for scene {scene_num}: {final_image_prompt}")

        except Exception as e:
            log.append(f"Error generating image prompt for scene {scene_num}: {e}")
            print(f"Error generating image prompt for scene {scene_num}: {e}")
            scene['image_prompt'] = "Error generating image prompt."

        updated_scenes.append(scene)

    return {"scenes": updated_scenes, "processing_log": log}


def generate_images_for_scenes(state: StoryAnalysisState) -> StoryAnalysisState:
    """
    Node to generate images based on prompts using the Google GenAI API
    and save them locally.
    """
    log = state.get("processing_log", [])
    log.append("Generating images using Google GenAI...")
    print("--- Generating Images (using Google GenAI) ---")
    scenes = state.get("scenes", [])
    updated_scenes = []

    # Ensure the GenAI client was initialized
    if not genai_client:
         log.append("Google GenAI Image Generation client not available. Skipping image generation.")
         print("‚ö†Ô∏è Google GenAI Image Generation client not available. Skipping image generation.")
         # Add existing scenes back without image URLs
         for scene in scenes:
             scene['image_url'] = None
             updated_scenes.append(scene)
         return {"scenes": updated_scenes, "processing_log": log}

    # Create an output directory if it doesn't exist
    output_dir = "./generated_images"
    os.makedirs(output_dir, exist_ok=True)
    print(f"Image output directory: {output_dir}")

    for scene in scenes:
        image_prompt = scene.get('image_prompt')
        scene_num = scene.get('scene_number', 'N/A')
        generated_image_path = None # Initialize path for this scene

        if image_prompt and "Error" not in image_prompt:
            print(f"Attempting image generation for scene {scene_num}...")
            try:
                # Use the prompt directly as contents
                contents = image_prompt

                # Call the Google GenAI API for image generation using client.models.generate_content
                response = genai_client.models.generate_content(
                            model="gemini-2.0-flash-exp-image-generation", # Specific model for image generation
                            contents=contents,
                            config=types.GenerateContentConfig(
                              response_modalities=['Text', 'Image'], # Both Text and Image as per https://ai.google.dev/gemini-api/docs/image-generation#gemini
                            )
                        )
                time.sleep(8) #Gemini-2.0-flash has rate limit of 10 per minute
                # Process the response to find and save the image
                image_saved = False
                # Check if candidates exist and have content parts
                if response.candidates and hasattr(response.candidates[0], 'content') and hasattr(response.candidates[0].content, 'parts'):
                    for part in response.candidates[0].content.parts:
                        # Check if the part has inline_data and if that data has a 'data' attribute
                        if hasattr(part, 'inline_data') and hasattr(part.inline_data, 'data'):
                            print(f"Image data received for scene {scene_num}.")
                            image_data = part.inline_data.data
                            try:
                                image = Image.open(io.BytesIO(image_data))
                                # Define file path
                                file_name = f"scene_{scene_num}_image.png"
                                generated_image_path = os.path.join(output_dir, file_name)
                                # Save the image
                                image.save(generated_image_path)
                                print(f"Saved image for scene {scene_num} to: {generated_image_path}")
                                log.append(f"Generated and saved image for scene {scene_num} to {generated_image_path}")
                                image_saved = True
                                break # Assuming only one image part per scene
                            except Exception as img_err:
                                log.append(f"Error processing/saving image data for scene {scene_num}: {img_err}")
                                print(f"Error processing/saving image data for scene {scene_num}: {img_err}")
                                # Keep generated_image_path as None

                if not image_saved:
                     log.append(f"No valid image data found or saved in response for scene {scene_num}.")
                     print(f"Warning: No valid image data found or saved in API response for scene {scene_num}.")

            except Exception as e:
                log.append(f"Error during image generation API call for scene {scene_num}: {e}")
                print(f"Error during image generation API call for scene {scene_num}: {e}")
                # Keep generated_image_path as None

        else:
            log.append(f"Skipping image generation for scene {scene_num} due to missing or error in prompt.")
            print(f"Skipping image generation for scene {scene_num} (invalid prompt).")
            # Keep generated_image_path as None

        # Update the scene info with the file path (or None if failed)
        scene['image_url'] = generated_image_path
        updated_scenes.append(scene)

    return {"scenes": updated_scenes, "processing_log": log}

# ---  Audio Generation ---

# def split_sentences(text): #Not needed for the current TTS model
#     '''
#     Given a text, return a list of sentences.
#     '''
#     sentences = re.split(r'\. |\.\n|\.\n\n|;', text.replace("\n", " ").strip())

#     ### Strip whitespace from each sentence
#     sentences = [
#         sentence.strip() + ".."
#         for sentence in sentences
#     ]

#     sentences = list(filter(None, sentences)) #Remove empty string
#     return sentences

def generate_audio_for_scenes(state: StoryAnalysisState) -> StoryAnalysisState:
    """
    Node to generate audio for a scene
    """
    log = state.get("processing_log", [])
    log.append("Generating audio ...")
    print("--- Generating Audio ---")
    scenes = state.get("scenes", [])
    updated_scenes = []

    # Create an output directory if it doesn't exist
    output_dir = "./generated_audio"
    os.makedirs(output_dir, exist_ok=True)
    print(f"Audio output directory: {output_dir}")

    for scene in scenes:
        scene_text = scene.get('scene_text')
        scene_num = scene.get('scene_number', 'N/A')
        scene_tone = scene.get('tone') # Get tone for potential use
        generated_audio_path = None # Initialize path for this scene

        if scene_text:
            print(f"Audio generation for scene {scene_num}...")
            try:


                # silence = np.zeros(int(0.25 * SAMPLE_RATE))
                # for sentence in split_sentences(scene_text):
                    # semantic_tokens = generate_text_semantic(
                    #                             sentence,
                    #                             history_prompt=SPEAKER,
                    #                             temp=GEN_TEMP,
                    #                             min_eos_p=0.05,
                    #                             silent = True
                    #                         )

                    # audio_array += [semantic_to_waveform(semantic_tokens, history_prompt=SPEAKER,),silence]
                    # generated_array = generate_audio(sentence, history_prompt=SPEAKER, text_temp = 0.1, waveform_temp=0.1)
                    # audio_array += [generated_array]
                # audio_output_combines = np.concatenate(generation.sequences.cpu().numpy())
                # audio_output_combines = np.concatenate(audio_array)
                file_name = f"scene_{scene_num}_audio.wav"

                generated_audio_path = os.path.join(output_dir, file_name)
                tts.tts_to_file(text=scene_text, file_path=generated_audio_path)
                torch.cuda.empty_cache()
                gc.collect()


                # ==========================================================

                print(f"Saving audio for scene {scene_num} to: {generated_audio_path}")
                log.append(f"Audio generation for scene {scene_num}. Path: {generated_audio_path}")

            except Exception as e:
                log.append(f"Error during audio generation for scene {scene_num}: {e}")
                print(f"Error during audio generation for scene {scene_num}: {e}")
                generated_audio_path = None # Indicate failure
        else:
            log.append(f"Skipping audio generation for scene {scene_num} due to missing scene text.")
            print(f"Skipping audio generation for scene {scene_num} (missing text).")
            generated_audio_path = None # Ensure path is None if text is missing

        # Update the scene info with the file path (or None if failed)
        scene['audio_url'] = generated_audio_path
        updated_scenes.append(scene)

    return {"scenes": updated_scenes, "processing_log": log}

"""## Build the Graph"""

# --- 3. Build the Graph ---

workflow = StateGraph(StoryAnalysisState)

# Add the nodes
workflow.add_node("read_story", read_story)
workflow.add_node("analyze_characters", analyze_characters)
workflow.add_node("generate_descriptions", generate_missing_descriptions)
workflow.add_node("analyze_scenes", analyze_scenes)
workflow.add_node("determine_style", determine_overall_style)
workflow.add_node("generate_image_prompts", generate_image_prompts)
workflow.add_node("generate_images", generate_images_for_scenes)
workflow.add_node("generate_audio", generate_audio_for_scenes)

# Define the edges (flow)
workflow.set_entry_point("read_story")
workflow.add_edge("read_story", "analyze_characters")
workflow.add_edge("analyze_characters", "generate_descriptions")
workflow.add_edge("generate_descriptions", "analyze_scenes")
workflow.add_edge("analyze_scenes", "determine_style")
workflow.add_edge("determine_style", "generate_image_prompts")
workflow.add_edge("generate_image_prompts", "generate_images")
workflow.add_edge("generate_images", "generate_audio")
workflow.add_edge("generate_audio", END)

# Compile the graph
app = workflow.compile()

"""## Run the graph"""

# --- 4. Run the Graph ---

# Provide the story text
# Short story about the moon landing generated by copilot for testing
# Replace with your own short story here
story = """
The countdown had begun.
Inside the towering Saturn V rocket, Neil Armstrong, Buzz Aldrin, and Michael Collins sat strapped into their seats, their hands steady despite the enormity of what lay ahead. The control room at Kennedy Space Center was a flurry of voices, confirming final checks.
"Three..., two..., one..."
A roar erupted as the engines ignited. The ground trembled. Smoke and fire billowed from the launch pad, and then‚Äîliftoff. The force pressed them back into their seats as the rocket climbed, breaking free from Earth's pull, carrying them into the void.

Hours passed, then days. They hurtled through space, watching Earth shrink behind them. It was a blue oasis in the darkness, fragile and breathtaking.

Finally, the moon grew closer, its cratered surface waiting. The Eagle Lunar Module detached, descending carefully.

Armstrong took that fateful step, his words echoing across history. Aldrin joined him, and together, they walked where no human had ever walked before.

Later, with experiments completed and footprints imprinted in the dust, they climbed back aboard, leaving the moon behind. The ascent module fired, propelling them back toward their waiting ship.

And as they drifted home, gazing at the Earth suspended in infinite black, they knew: humanity had touched the stars, and it would never be the same again.
"""
# Define the initial input for the graph
initial_state = {"story_text": story}

# Execute the graph flow
print("\n--- Running Graph ---")
# Ensure LLM and GenAI client are available before running
if llm and genai_client:
    final_state = app.invoke(initial_state)

    print("\n--- Final State ---")
    # Use json.dumps for pretty printing the dictionary
    print(json.dumps(final_state, indent=2))
else:
    print("\n--- Graph Execution Skipped (LLM or GenAI client not initialized) ---")

"""# Building the video"""

import os
from moviepy.editor import ImageClip, AudioFileClip, concatenate_videoclips


final_state = final_state
# -----------------------------------------------------


output_video_filename = "final_story_video.mp4"
scene_clips = [] # To store video clips for each scene

print("Starting video creation process...")

# --- Iterate through scenes and create video clips ---
if 'scenes' in final_state and isinstance(final_state['scenes'], list):
    for scene in final_state['scenes']:
        scene_num = scene.get('scene_number', 'Unknown')
        image_path = scene.get('image_url')
        audio_path = scene.get('audio_url')

        print(f"\nProcessing Scene {scene_num}...")
        print(f"  Image Path: {image_path}")
        print(f"  Audio Path: {audio_path}")

        # --- Validate paths and files ---
        if not image_path or not os.path.exists(image_path):
            print(f"  Warning: Image file not found for scene {scene_num}. Skipping scene.")
            continue
        if not audio_path or not os.path.exists(audio_path):
            print(f"  Warning: Audio file not found for scene {scene_num}. Skipping scene.")
            continue

        try:
            # --- Load audio to get duration ---
            audio_clip = AudioFileClip(audio_path)
            scene_duration = audio_clip.duration
            if scene_duration <= 0:
                 print(f"  Warning: Audio duration is zero or negative for scene {scene_num}. Skipping scene.")
                 audio_clip.close() # Close the clip
                 continue
            print(f"  Audio Duration: {scene_duration:.2f} seconds")

            # --- Create image clip ---
            # Set image duration to match audio duration
            img_clip = ImageClip(image_path).set_duration(scene_duration)

            # --- Set audio for the image clip ---
            video_clip = img_clip.set_audio(audio_clip)

            # Add the completed scene clip to our list
            scene_clips.append(video_clip)
            print(f"  Successfully created video clip for scene {scene_num}.")

        except Exception as e:
            print(f"  Error processing scene {scene_num}: {e}")
            # Ensure clips are closed if an error occurs mid-processing
            if 'audio_clip' in locals() and hasattr(audio_clip, 'close'):
                audio_clip.close()
            if 'img_clip' in locals() and hasattr(img_clip, 'close'):
                img_clip.close()
            if 'video_clip' in locals() and hasattr(video_clip, 'close'):
                video_clip.close()


else:
    print("Error: 'scenes' key not found or is not a list in final_state.")

# --- Concatenate all scene clips ---
if scene_clips:
    print(f"\nConcatenating {len(scene_clips)} scene clips...")
    try:
        final_clip = concatenate_videoclips(scene_clips, method="compose")

        # --- Write the final video file ---
        print(f"Writing final video to {output_video_filename}...")

        final_clip.write_videofile(
            output_video_filename,
            codec='libx264',    # Common video codec
            # audio_codec='pcm_s32le', # Common audio codec
            fps=24,
            threads=4,         # Adjust based on your CPU cores
            logger='bar'       # Show progress bar
        )
        print("Final video saved successfully!")

        # --- Close final clip ---
        final_clip.close()

    except Exception as e:
        print(f"Error during concatenation or writing final video: {e}")
else:
    print("No valid scene clips were created. Final video cannot be generated.")

# --- Cleanup: Close any remaining clips in the list ---
print("Cleaning up...")
for clip in scene_clips:
     if hasattr(clip, 'close'):
         clip.close()

print("Video creation process finished.")

"""# Video output"""

from IPython.display import Video

Video("/kaggle/working/final_story_video.mp4", embed=True)

"""## ‚ö†Ô∏è Limitations of the Current Implementation

While this project demonstrates a promising approach, there are several limitations to consider in its current implementation:

* **Audio Generation Speed and Quality:** The current Text-to-Speech implementation can be slow, particularly when generating longer audio segments. Generating audio sentence by sentence, while potentially improving perceived responsiveness, introduces overhead and disrupting the flow of the story.
* **Audio Quality and Control:** TTS APIs offers limited fine-grained control over crucial aspects like voice emotion, prosody, and the ability to distinctly differentiate between narration and dialogue, especially when compared to more advanced commercial TTS APIs. Furthermore, concatenating audio generated at the sentence level can sometimes result in unnatural pauses or an uneven flow.
* **Image Consistency:** Achieving perfect visual consistency for characters (appearance, clothing) and objects across independently generated scenes remains a significant challenge for current image generation models when relying solely on text prompts, even with the inclusion of a style descriptor.
* **Scene Segmentation Accuracy:** The scene analysis performed by the LLM, while generally effective, may occasionally produce segmentations that deviate from human interpretation or miss more nuanced scene transitions within the story.
* **Robust Error Handling:** While basic error handling is implemented (e.g., for sentence generation in TTS), the workflow could benefit from more comprehensive error handling to gracefully manage potential API failures or unexpected outputs from the various models.
* **Lack of Final Video Output:** The current script focuses on generating individual image and audio assets, with their file paths stored in `image_url` and `audio_url`. A separate post-processing step or script (such as the `moviepy` example mentioned) is necessary to combine these assets into a final, playable video file.
* **Scalability, Rate Limits, and Cost:** Processing very long stories can significantly increase execution time, API costs (due to token usage and the number of image and audio generation calls), and the likelihood of encountering API rate limits. Additionally, memory usage could become a concern for extremely long texts.

## üîÑ Exploring Alternatives

To address some of the limitations mentioned above and potentially enhance the quality and efficiency of the AI Story Visualizer, consider exploring the following alternative libraries and APIs:

### Image Generation Libraries/APIs:

* **[OpenAI DALL-E 3](https://openai.com/index/dall-e-3/)**: Known for its high-quality image generation and improved ability to follow complex instructions.
* **[Stable Diffusion](https://stability.ai/)**: A powerful open-source model offering flexibility and customization options.
* **[Amazon Nova](https://aws.amazon.com/ai/generative-ai/nova/)**: Amazon's image generation service, potentially offering seamless integration within the AWS ecosystem.
* **[Google Imagen-3](https://deepmind.google/technologies/imagen-3/)**: Google's latest image generation model, promising enhanced realism and detail.

### Audio Generation (TTS) Libraries/APIs:

* **[Google Cloud Text-to-Speech](https://cloud.google.com/text-to-speech?hl=en)**: Offers high-quality, natural-sounding voices with various customization options, including voice selection and speaking styles.
* **[ElevenLabs](https://elevenlabs.io/)**: Specializes in highly realistic and expressive voice cloning and text-to-speech capabilities.
* **[Microsoft Azure AI Speech](https://azure.microsoft.com/en-us/products/ai-services/ai-speech)**: Provides a comprehensive suite of speech services, including high-quality TTS with a wide range of voices and languages.
* **[Amazon Polly](https://aws.amazon.com/polly/)**: Offers lifelike text-to-speech voices and supports various languages.
* **[Coqui TTS (original github is no longer being maintained).](https://github.com/idiap/coqui-ai-TTS)**: While the original repository is not actively maintained, it was a popular open-source TTS library and might have community forks or related projects worth exploring.
* **[Piper TTS](https://github.com/rhasspy/piper)**: A lightweight and fast open-source TTS engine focused on speed and efficiency, suitable for resource-constrained environments.

## üõ†Ô∏è Future ideas for Improvement

This section outlines potential areas for improvement in the current workflow.

### Improving Test Prompts for GenAI Calls

Crafting effective prompts is crucial for achieving the desired output from each GenAI model in the workflow. Here are some strategies for improving prompts at different stages:

* **Character Analysis (`analyze_characters`):**
    * **Provide context and examples:** Include examples of well-formed character descriptions to guide the LLM.
    * **Specify the level of detail:** Indicate whether brief summaries or more in-depth analyses are required.

* **Scene Analysis (`analyze_scenes`):**
    * **Define what constitutes a "scene":** Provide a clear definition of a scene (e.g., a change in location, time, or main characters).
    * **Encourage consideration of narrative flow:** Prompt the LLM to consider the overall narrative arc when segmenting scenes.

* **Image Generation:**

    * **Experiment with negative prompts:** Use negative prompts to specify elements that should *not* be included in the generated image.
    * **Consider aspect ratio and resolution:** Include these specifications in the prompt.

* **Text-to-Speech (TTS):**
    * **Specify the desired voice and tone, if the TTS API allows,:** Include instructions on the type of voice (e.g., male, female, child), accent, and desired emotional tone (e.g., "narrative," "excited," "somber").
    * **Control prosody and pacing:** Some advanced TTS APIs allow for fine-tuning of speech rate, pauses, and emphasis. Explore these options to improve the naturalness of the audio.

### Incorporating Evaluation Nodes

To ensure the quality and consistency of the generated content, integrating evaluation nodes into the LangGraph workflow is crucial. These nodes would assess the output of various GenAI calls based on predefined criteria. Here are some ideas for evaluation:

* **Character Analysis Evaluation:**
    * **Consistency Check:** Evaluate if character descriptions are consistent across different scenes where the character appears.
    * **Relevance to the Story:** Assess if the extracted character traits and descriptions are accurate and relevant to the provided story text.
    * **Completeness:** Check if the required fields in the JSON output are populated and contain meaningful information.

* **Scene Analysis Evaluation:**
    * **Segmentation Accuracy:** Evaluate if the identified scene breaks align with human intuition and narrative flow. This could involve comparing the automated segmentation with a manually annotated version for a subset of stories.
    * **Summary Quality:** Assess the accuracy and conciseness of the scene summaries generated by the LLM.

* **Image Generation Evaluation:**
    * **Prompt Adherence:** Evaluate how well the generated image aligns with the provided prompt, including the described objects, characters, actions, style, and mood.
    * **Visual Consistency:** If a character appears in multiple scenes, evaluate the visual consistency of their representation across different generated images. This is a challenging task but could involve techniques like feature extraction and comparison.
    * **Aesthetic Quality:** While subjective, consider incorporating metrics or human feedback on the overall visual appeal and quality of the generated images.

* **Text-to-Speech Evaluation:**
    * **Audio Quality:** Assess the clarity, naturalness, and absence of artifacts in the generated audio.
    * **Voice Appropriateness:** Evaluate if the chosen voice and tone are suitable for the content being narrated.
    * **Synchronization with Text:** Although not explicitly an evaluation node within the LangGraph flow in this initial stage, ensuring the generated audio aligns with the original text is crucial.

**Methods for Implementing Evaluation Nodes:**

* **LLM-based Evaluation:** Utilize another LLM (potentially a smaller, faster model) to evaluate the output of previous nodes based on specific prompts and criteria. This allows for automated and flexible evaluation.
* **Rule-based Evaluation:** Define specific rules and checks to assess the output format and content. For example, ensuring the JSON output is valid and contains all required fields.

"""